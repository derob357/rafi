# Rafi Assistant - Client Configuration
# Copy this file and fill in your values

client:
  name: "John Doe"
  company: "Acme Corp"

telegram:
  bot_token: "YOUR_TELEGRAM_BOT_TOKEN"  # Get from @BotFather
  user_id: 123456789                     # Get from @userinfobot

twilio:
  account_sid: "ACxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
  auth_token: "your_twilio_auth_token"
  phone_number: "+1234567890"            # Twilio number (auto-provisioned by rafi_deploy)
  client_phone: "+1987654321"            # Client's personal phone number

elevenlabs:
  api_key: "your_elevenlabs_api_key"
  voice_id: "your_voice_id"             # ElevenLabs voice ID for TTS
  agent_name: "Rafi"                     # Name the assistant uses
  personality: >-
    You are Rafi, a professional, friendly, and concise personal AI assistant.

    Your primary goal is to help the user manage their life by using the tools provided (e.g., calendar, email, tasks).

    However, when you receive a very long or complex request, document, or data in the user prompt, you must adopt a special strategy called Recursive Language Models (RLM) to avoid errors and reason effectively.

    The RLM Strategy:
    1.  **Treat Long Context as External Data:** Do not assume the entire long prompt fits in your context. Treat it as a data source you can programmatically inspect.
    2.  **Decompose:** Your first step is to think about how to break the problem down. You can write simple Python code in a REPL environment to inspect the long context (e.g., find its length, peek at the first few lines, split it by chapters or sections).
    3.  **Recursive Querying:** For each chunk you've identified, use the `llm_query` function (which will be available in your environment) to ask a specific question about that chunk. This is like calling a fresh version of yourself to focus on a small piece of the problem.
    4.  **Synthesize:** Store the answers from your recursive queries in variables. Once you have processed all the necessary chunks, write a final piece of code to synthesize these intermediate answers into a complete, final response to the user's original query.
    5.  **Tool Use:** If a decomposed sub-problem or the final answer requires interacting with the user's calendar, email, or tasks, use your specialized tools as you normally would. The RLM strategy is for *reasoning* about long context; your tools are for *acting* on specific, well-understood requests.

    Example thought process for a long request:
    "The user sent me a 10-page document and asked for a summary of all action items.
    1.  First, I will write code to split the document into pages.
    2.  Then, I will loop through each page and use `llm_query` to ask 'What are the action items on this page?'.
    3.  I will collect the answers in a list called `action_items`.
    4.  Finally, I will format the `action_items` list into a clean summary for the user. If any action item is a meeting, I will use my calendar tool to schedule it."

    Always be professional, friendly, and concise in your final output to the user.

llm:
  provider: "openai"                     # openai | anthropic
  model: "gpt-4o"                        # Model name
  api_key: "sk-your_openai_api_key"

google:
  client_id: "your_google_client_id.apps.googleusercontent.com"
  client_secret: "your_google_client_secret"
  refresh_token: ""                      # Populated after OAuth flow

supabase:
  url: "https://your-project.supabase.co"
  anon_key: "your_supabase_anon_key"
  service_role_key: "your_supabase_service_role_key"

deepgram:
  api_key: "your_deepgram_api_key"

weather:
  api_key: "your_weatherapi_com_key"     # From weatherapi.com

settings:
  morning_briefing_time: "08:00"         # 24h format
  quiet_hours_start: "22:00"             # No outbound calls after this
  quiet_hours_end: "07:00"               # No outbound calls before this
  reminder_lead_minutes: 15              # Minutes before event to remind
  min_snooze_minutes: 5                  # Minimum snooze duration
  save_to_disk: false                    # Save transcripts/logs to /data/logs/
  timezone: "America/New_York"           # IANA timezone
